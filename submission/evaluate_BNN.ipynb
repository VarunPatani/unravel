{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40c05ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/miniconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0561b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration of proper device for training purposes\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "# Path to the final model\n",
    "MODEL_PATH = \"bandit_bnn_model_final.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dea3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the essential data for the model\n",
    "order_data = pd.read_csv(\"../Datasets/order_data_cleaned.csv\")\n",
    "customer_data = pd.read_csv(\"../Datasets/customer_data_cleaned.csv\")\n",
    "test_data = pd.read_csv(\"../Datasets/test_data_question.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3caf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts the json_string in order_data (cleaned data) and returns a list of all items inside of it\n",
    "def extract_items(order_json_string):\n",
    "    try:\n",
    "        data = json.loads(order_json_string)\n",
    "        return [item[\"item_name\"] for item in data[\"orders\"][0][\"item_details\"]]\n",
    "    except (json.JSONDecodeError, IndexError, KeyError):\n",
    "        return []\n",
    "\n",
    "\n",
    "# This creates a column called \"item_list\" which contains all the item's in a list format included in that order\n",
    "order_data[\"item_list\"] = order_data[\"ORDERS\"].apply(extract_items)\n",
    "# this creates a set (to remove duplicates) to get all unique items in order_data\n",
    "all_items_in_orders = set(\n",
    "    [item for sublist in order_data[\"item_list\"] for item in sublist]\n",
    ")\n",
    "# this creates a set (to remove duplicates) to get all unique items in test_data\n",
    "all_items_in_test = (\n",
    "    set(test_data[\"item1\"].unique())\n",
    "    | set(test_data[\"item2\"].unique())\n",
    "    | set(test_data[\"item3\"].unique())\n",
    ")\n",
    "# this is basically a union list of all items from both sets created to get an idea of the overall number of unique items in the complete entire database\n",
    "ARM_VOCABULARY = sorted(list(all_items_in_orders | all_items_in_test))\n",
    "N_ARMS = len(ARM_VOCABULARY)\n",
    "\n",
    "# CONTEXT_FEATURES is a dictionary that maps feature names to their unique values\n",
    "# this basically stores all the unique different feature values for features that are included for our valuation/prediction\n",
    "CONTEXT_FEATURES = {\n",
    "    \"CUSTOMER_TYPE\": sorted(\n",
    "        customer_data[\"CUSTOMER_TYPE\"].astype(str).unique().tolist()\n",
    "    ),\n",
    "    \"STORE_NUMBER\": sorted(order_data[\"STORE_NUMBER\"].unique().tolist()),\n",
    "    \"ITEMS\": ARM_VOCABULARY,\n",
    "}\n",
    "# this creates a OneHotEncoder for the CUSTOMER_TYPE feature\n",
    "customer_type_encoder = OneHotEncoder(\n",
    "    categories=[CONTEXT_FEATURES[\"CUSTOMER_TYPE\"]],\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False,\n",
    ")\n",
    "# this creates a OneHotEncoder for the STORE_NUMBER feature\n",
    "store_number_encoder = OneHotEncoder(\n",
    "    categories=[CONTEXT_FEATURES[\"STORE_NUMBER\"]],\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False,\n",
    ")\n",
    "# the encoders are then fitted onto the respective feature sets which are converted into a 2D array of 1 column as OneHotEncoder expects input in 2D format\n",
    "customer_type_encoder.fit(np.array(CONTEXT_FEATURES[\"CUSTOMER_TYPE\"]).reshape(-1, 1))\n",
    "store_number_encoder.fit(np.array(CONTEXT_FEATURES[\"STORE_NUMBER\"]).reshape(-1, 1))\n",
    "\n",
    "# this creates a dictionary where each item is mapped to a unique index value (Dict so that O(1) access is possible)\n",
    "ARM_MAP = {item: i for i, item in enumerate(ARM_VOCABULARY)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7420fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in context vector: 182\n"
     ]
    }
   ],
   "source": [
    "# Main function that takes customer type, store number, and items in cart as inputs and returns a combined context vector that is provided as imput to the model for its training purposes.\n",
    "def get_context_vector(customer_type, store_number, items_in_cart):\n",
    "    customer_vec = customer_type_encoder.transform(np.array([[customer_type]]))\n",
    "    store_vec = store_number_encoder.transform(np.array([[store_number]]))\n",
    "    items_vec = np.zeros((1, len(CONTEXT_FEATURES[\"ITEMS\"])))\n",
    "    # this loop sets the appropriate indices in the items_vec to 1 for each item in the cart\n",
    "    for item in items_in_cart:\n",
    "        if item in ARM_MAP:\n",
    "            items_vec[0, ARM_MAP[item]] = 1\n",
    "    # returns the concatenated version of all feature vectors to be inputted to model.\n",
    "    return np.concatenate(\n",
    "        [np.array([[1]]), customer_vec, store_vec, items_vec], axis=1\n",
    "    ).flatten()\n",
    "\n",
    "\n",
    "# a dummy run to check proper output\n",
    "dummy_context = get_context_vector(\"Guest\", order_data[\"STORE_NUMBER\"].iloc[0], [])\n",
    "N_FEATURES = len(dummy_context)\n",
    "print(f\"Number of features in context vector: {N_FEATURES}\")\n",
    "# uncomment below line to check output\n",
    "# dummy_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c86b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from bandit_bnn_model_final.pth...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10640/4027396774.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# Custom Bandit Neural Network created for this specific use case.\n",
    "# uses Dropout to ensure regularization\n",
    "class BanditBNN(nn.Module):\n",
    "    def __init__(self, n_features, n_arms, dropout_rate=0.5):\n",
    "        super(BanditBNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_features, 160)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.hidden2 = nn.Linear(160, 140)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.output = nn.Linear(140, n_arms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# Loading the trained model\n",
    "model = BanditBNN(N_FEATURES, N_ARMS).to(DEVICE)\n",
    "# Check for path\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"Loading trained model from {MODEL_PATH}...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\n",
    "        f\"Error: Model file not found at {MODEL_PATH}. Please train and save the model first.\"\n",
    "    )\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d84841ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns 'top_n' item recommendations based on customer context and items in cart along with the store number.\n",
    "def get_recommendations_gpu(customer_type, store_number, items_in_cart, top_n=3):\n",
    "    context_vector = get_context_vector(customer_type, store_number, items_in_cart)\n",
    "    context_tensor = torch.FloatTensor(context_vector).to(DEVICE)\n",
    "\n",
    "    # Enable dropout for Monte Carlo Dropout which resembles Thompson Sampling.\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        scores = model(context_tensor)\n",
    "\n",
    "    # Get top_n recommendations\n",
    "    _, top_indices = torch.topk(scores, top_n)\n",
    "\n",
    "    # Get the item names for the top_n indices\n",
    "    return [ARM_VOCABULARY[i] for i in top_indices.cpu().numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e153160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BNN Test Set: 100%|██████████| 1000/1000 [00:00<00:00, 1949.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BNN Evaluation Complete ---\n",
      "Recall@3 Score: 0.3020 (302/1000 hits)\n",
      "Average Inference Time per Order: 0.47 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST FOR CONTEXT_ITEMS = 2\n",
    "\n",
    "# THIS CELL HERE TESTS THE MODELS RECALL@3 SCORE CAPABILITY ON THE TEST_DATA WITH 2 CART ITEMS AS CONTEXT (item_1 and item_2) AND STORING THE LAST ITEM (item_3) AS GROUND TRUTH TO TEST FOR RECALL@3\n",
    "\n",
    "# Variable to count hits\n",
    "hits = 0\n",
    "total = len(test_data)\n",
    "# a list to get the average inference time taken to process one order\n",
    "inference_times = []\n",
    "\n",
    "if total > 0:\n",
    "    for _, row in tqdm(\n",
    "        test_data.iterrows(), total=total, desc=\"Evaluating BNN Test Set\"\n",
    "    ):\n",
    "        # storing the first two items as context\n",
    "        context_cart = [row[\"item1\"], row[\"item2\"]]\n",
    "        ground_truth_item = row[\"item3\"]\n",
    "\n",
    "        # Measure Inference Time\n",
    "        start_time = time.perf_counter()\n",
    "        recommendations = get_recommendations_gpu(\n",
    "            row[\"CUSTOMER_TYPE\"], row[\"STORE_NUMBER\"], context_cart\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        if ground_truth_item in recommendations:\n",
    "            hits += 1\n",
    "\n",
    "    # --- Calculate and Print Results ---\n",
    "    recall_at_3 = hits / total\n",
    "    avg_inference_time_ms = (sum(inference_times) / total) * 1000\n",
    "\n",
    "    print(\"\\n--- BNN Evaluation Complete ---\")\n",
    "    print(f\"Recall@3 Score: {recall_at_3:.4f} ({hits}/{total} hits)\")\n",
    "    print(f\"Average Inference Time per Order: {avg_inference_time_ms:.2f} ms\")\n",
    "else:\n",
    "    print(\"test_data_question.csv is empty. No evaluation performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9ba26",
   "metadata": {},
   "source": [
    "# Creating the .xlsx and .csv file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9276ce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Recommendations: 100%|██████████| 1000/1000 [00:00<00:00, 1903.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation generation complete.\n",
      "\n",
      "Saving submission file to submission.xlsx...\n",
      "Saving submission file to submission.csv...\n",
      "\n",
      "Process finished successfully.\n",
      "   CUSTOMER_ID  STORE_NUMBER    ORDER_ID ORDER_CHANNEL_NAME  \\\n",
      "0    997177535          4915  9351345556            Digital   \n",
      "1    345593831           949  3595377080            Digital   \n",
      "2    160955031          2249  4071757785            Digital   \n",
      "3    890671991          4154  3931766769            Digital   \n",
      "4     73989021          4094  3739700809            Digital   \n",
      "\n",
      "  ORDER_SUBCHANNEL_NAME ORDER_OCCASION_NAME CUSTOMER_TYPE  \\\n",
      "0                   WWT                ToGo         Guest   \n",
      "1                   WWT                ToGo    Registered   \n",
      "2                   WWT                ToGo         Guest   \n",
      "3                   WWT                ToGo         Guest   \n",
      "4                   WWT                ToGo    Registered   \n",
      "\n",
      "                      item1                item2                     item3  \\\n",
      "0         Chicken Sub Combo  Ranch Dip - Regular   10 pc Spicy Wings Combo   \n",
      "1     Regular Buffalo Fries    10 pc Spicy Wings  3 pc Crispy Strips Combo   \n",
      "2       Large Buffalo Fries    10 pc Spicy Wings       Ranch Dip - Regular   \n",
      "3  6 pc Grilled Wings Combo  20 pc Grilled Wings        Fried Corn - Large   \n",
      "4     Regular Buffalo Fries  20 pc Grilled Wings         Ranch Dip - Large   \n",
      "\n",
      "            RECOMMENDATION_1       RECOMMENDATION_2        RECOMMENDATION_3  \n",
      "0  10 pc Grilled Wings Combo     2 pc Crispy Strips  6 pc Spicy Wings Combo  \n",
      "1        Ranch Dip - Regular      Ranch Dip - Large      2 pc Crispy Strips  \n",
      "2        Ranch Dip - Regular    10 pc Grilled Wings   Regular Buffalo Fries  \n",
      "3        Ranch Dip - Regular      Ranch Dip - Large     Large Buffalo Fries  \n",
      "4         2 pc Crispy Strips  Regular Buffalo Fries     Ranch Dip - Regular  \n"
     ]
    }
   ],
   "source": [
    "all_recommendations = []\n",
    "for _, row in tqdm(\n",
    "    test_data.iterrows(), total=len(test_data), desc=\"Generating Recommendations\"\n",
    "):\n",
    "    context_cart = [row[\"item1\"], row[\"item2\"], row[\"item3\"]]\n",
    "\n",
    "    recommendations = get_recommendations_gpu(\n",
    "        row[\"CUSTOMER_TYPE\"], row[\"STORE_NUMBER\"], context_cart\n",
    "    )\n",
    "    all_recommendations.append(recommendations)\n",
    "\n",
    "test_data[\"RECOMMENDATION_1\"] = [rec[0] for rec in all_recommendations]\n",
    "test_data[\"RECOMMENDATION_2\"] = [rec[1] for rec in all_recommendations]\n",
    "test_data[\"RECOMMENDATION_3\"] = [rec[2] for rec in all_recommendations]\n",
    "\n",
    "print(\"Recommendation generation complete.\")\n",
    "\n",
    "output_path_xlsx = os.path.join(\"submission.xlsx\")\n",
    "output_path_csv = os.path.join(\"submission.csv\")\n",
    "\n",
    "submission_df = test_data[\n",
    "    [\n",
    "        \"CUSTOMER_ID\",\n",
    "        \"STORE_NUMBER\",\n",
    "        \"ORDER_ID\",\n",
    "        \"ORDER_CHANNEL_NAME\",\n",
    "        \"ORDER_SUBCHANNEL_NAME\",\n",
    "        \"ORDER_OCCASION_NAME\",\n",
    "        \"CUSTOMER_TYPE\",\n",
    "        \"item1\",\n",
    "        \"item2\",\n",
    "        \"item3\",\n",
    "        \"RECOMMENDATION_1\",\n",
    "        \"RECOMMENDATION_2\",\n",
    "        \"RECOMMENDATION_3\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nSaving submission file to {output_path_xlsx}...\")\n",
    "submission_df.to_excel(output_path_xlsx, index=False)\n",
    "\n",
    "print(f\"Saving submission file to {output_path_csv}...\")\n",
    "submission_df.to_csv(output_path_csv, index=False)\n",
    "\n",
    "print(\"\\nProcess finished successfully.\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
